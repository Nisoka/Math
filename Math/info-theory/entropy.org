* entropy

  信息 -- 实际上对应的是概率分布

  <信息论基础>
  信息论中的各种熵  http://blog.csdn.net/young_gy/article/details/69666014
  

** 信息量
   某个概率分布的实际上真实的描述了一个随机变量.
   而更进一步, 一个随机变量 实际上具有信息的概念. 某个随机变量X == x1 或者 x2时是两种状态, 具有不同的信息.
   但是怎么去度量一个随机变量在某个状态时 具有的信息的量?

   对于一个随机变量X (x1, x2, x3, x4)而言(不考虑其他因素, 整个世界只有一个变量时), 
   当X为最大概率的状态时,实际上携带的信息最少, 相反X为最小概率状态时,  携带了大信息量.
   随机变量为某个状态时的信息量 == 表达该状态所用的信息(bit等). 

   信息量 = -log(p(xi))
   
** 熵
   信息是一个相当抽象的概念, 很难用一个简单的定义将其理解.
   对于任何一个概率分布, 可以定义一个熵的量, 用来度量该概率分布的信息.
   某个概率分布的 信息量 的期望.
   *注意 熵 并不等于信息量，而是信息量的期望, 可以认为是不确定度*
   *熵也可以用来表示 描述该概率分布可能状态 所需编码的平均编码长度 -- 通信理论中常用的概念*

** 联合熵
   两个随机变量 确定为某个状态的信息量为-log(p(x,y)).
   对应的联合熵 是联合信息量的期望 H(X,Y)
   
** 条件熵
   一个随机变量 在另一个随机变量的状态确定时的信息量的期望
   H(X|Y)
   H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
    
** 相对熵  KL散度
   描述同一个随机变量时两个概率分布之间距离的一种度量。 统计学中是 似然比的对数的期望.
   D(p||q) 度量当真实分布为p, 而假设分布为q 时的无效性.
   
   P分布的熵为 H(p), 而当使用假设q来描述p时(机器学习中使用假设来逼近真实分布), 得到的熵是（就是交叉熵(假设熵)）
   H(q-p) = H(p) + D(p||q) = -SUM{ p(x) * log(q(x)) }
   
   D(p||q) = H(p) - H(q-p)
           = SUM{ p(x) * log( p(x) / q(x) ) }
   

   KL散度是两个概率分布P和Q差别的非对称性的度量。
   KL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 
   典型情况下，P表示数据的真实分布，Q 表示数据的理论分布，模型分布，或P的近似分布。

   相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。
   所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算KL散度就行了。
   另外，在多指标系统评估中，指标权重分配是一个重点和难点，通过相对熵可以处理。
   http://blog.csdn.net/acdreamers/article/details/44657745

** 互信息
   互信息I(X;Y) 
   联合分布p(x,y), 当使用乘积分布p(x)p(y)描述联合分布时的相对熵 - 叫做互信息 I(X;Y)
   也能描述, 当某个随机变量X, 在随机变量Y状态确定条件下,对随机变量X的熵的减少.
   即 互信息I(X;Y) 是给定Y状态下, X的不确定度(信息量期望，熵)的缩减量.

   I(X;Y) = D(p(x,y) || p(x)p(y))
          = H(X) - H(X|Y)
          = H(X) + H(Y) - H(X,Y)
          = I(Y;X)

   I(X;X) = H(X) - H(X|X) = H(X)  --- 自信息
          

** 交叉熵 -- 可以理解为假设近似熵
   当编码不一定完美时, 平均编码长度为多少
   当用假设分布q 去逼近真实分布p时的熵
   H(p||q) = H(p) + D(p||q)

   真实分布熵 + 相对熵 == 假设分布对真实分布的交叉熵.
   并且 因为互信息 D(p||q) 永远>0 . H(p||q) 永远> 真实熵
   当交叉熵 越小, p 越接近 q, 也就能够作为损失函数。



   1、熵的本质是香农信息量 log(1/p) 的期望；（参考了第一个答案）      
   H(p) = E[ log(1/p) ] = ∑ p_i * log(1/p_i)，是一个期望的计算，也是记录随机事件结果的平均编码长度；       
   为什么信息量 是 log(1/p) 呢？       
      因为：一个事件结果的出现概率越低，对其编码的bit长度就越长,以期在整个随机事件的无数次重复试验中，用最少的 bit 去记录整个实验历史。
      即无法压缩的表达，代表了真正的信息量。

   2、熵的本质的另一种解释：最短平均编码长度；    
      本质含义：编码方案完美时--完全按照信息量完美编码，最短平均编码长度(平均信息量 - 熵)的是多少
    
   3、交叉熵，则可以这样理解：
      使用了“估算”的编码后，得到的平均编码长度(可能不是最短的)
      p是真实概率分布，q是你以为的概率分布（可能不一致）；
      你以 q 去编码，编码方案 log(1/q_i)可能不是最优的；
      于是，平均编码长度 = ∑ p_i *log(1/q_i)，就是交叉熵；
      只有在估算的分布 q 完全正确时，平均编码长度才是最短的，交叉熵 = 熵.


    作者：张一山
    链接：https://www.zhihu.com/question/41252833/answer/140950659
    来源：知乎
    著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
    
*** 信息论中的各种熵
    http://blog.csdn.net/young_gy/article/details/69666014
    
    
    
** 信息熵的推导过程:
   王小龙回答:
   https://www.zhihu.com/question/24053383 
   "pattern recognition and machine learning"第一章第6节


